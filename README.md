# MLearningArbreDecisionRandomforest_Titanic

# ğŸ¯ Titanic - Analyse PrÃ©dictive de Survie

Ce projet applique des techniques de Machine Learning pour prÃ©dire la survie des passagers du Titanic en fonction de leurs caractÃ©ristiques (Ã¢ge, sexe, classe sociale, etc.). Il repose sur le dataset classique de Kaggle ["Titanic: Machine Learning from Disaster"](https://www.kaggle.com/c/titanic).

## ğŸ“‚ Contenu du projet

- ğŸ”„ PrÃ©traitement des donnÃ©es (imputation, encodage, suppression colonnes inutiles)
- ğŸ” Analyse exploratoire (visualisations, corrÃ©lations)
- ğŸ§  EntraÃ®nement de modÃ¨les :
  - `DecisionTreeClassifier`
  - `RandomForestClassifier`
- ğŸ“ˆ Ã‰valuation des modÃ¨les (Accuracy, PrÃ©cision, Rappel, F1, ROC)
- ğŸ§ª Optimisation avec `GridSearchCV`
- ğŸ“Š InterprÃ©tation des rÃ©sultats

---

## ğŸ“Œ Objectifs

- Comprendre l'impact des variables comme `Sex`, `Pclass`, `Fare`, `Title` sur la survie
- Comparer diffÃ©rents modÃ¨les d'apprentissage supervisÃ©
- Mettre en place une pipeline robuste de modÃ©lisation

---

## âš™ï¸ Technologies utilisÃ©es

- Python 3
- Pandas, NumPy
- Seaborn, Matplotlib
- Scikit-learn
- Graphviz (pour affichage de lâ€™arbre de dÃ©cision)

---

## ğŸ“Š AperÃ§u des rÃ©sultats

| ModÃ¨le                     | Accuracy (Test) |
|---------------------------|-----------------|
| DecisionTree (max_depth=4)| ~0.80           |
| RandomForest (optimisÃ©)   | **0.835** âœ…     |

- ğŸ¥‡ Variable la plus discriminante : **Sex**
- ğŸ› ï¸ Meilleurs hyperparamÃ¨tres :  
  `max_depth=10`, `min_samples_split=10`, `n_estimators=100`

---

## ğŸ§  Conclusion

- La **Random Forest** surpasse lâ€™arbre de dÃ©cision seul.
- Le modÃ¨le est stable et gÃ©nÃ©ralise bien.
- Certaines variables crÃ©Ã©es (`Title`, `FamilySize`, etc.) amÃ©liorent les performances.

---

## ğŸš€ Ã€ venir / pistes d'amÃ©lioration

- DÃ©ploiement avec Streamlit ou Flask


